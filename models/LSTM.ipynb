{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcKyre3FQlRK"
      },
      "source": [
        "<h2>LSTM</h2>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import json\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout, Embedding\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load intents\n",
        "data_file = open('intents.json').read()\n",
        "intents = json.loads(data_file)\n",
        "\n",
        "# Extract text and labels from intents\n",
        "texts = []\n",
        "labels = []\n",
        "\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        texts.append(pattern)\n",
        "        labels.append(intent['tag'])\n",
        "\n",
        "# Tokenize the text data\n",
        "max_words = 1000\n",
        "tokenizer = Tokenizer(num_words=max_words, lower=True, split=\" \")\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Pad sequences\n",
        "max_sequence_length = max([len(seq) for seq in sequences])\n",
        "data = pad_sequences(sequences, maxlen=max_sequence_length)\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "encoded_labels = le.fit_transform(labels)\n",
        "num_classes = len(le.classes_)\n",
        "\n",
        "# Define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 128, input_length=max_sequence_length))\n",
        "model.add(LSTM(128, return_sequences=True, kernel_regularizer=l2(0.001)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(LSTM(64, kernel_regularizer=l2(0.001)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax', kernel_regularizer=l2(0.001)))\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(lr=0.001)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "#model.fit(data, encoded_labels, validation_split=0.1, epochs=100, batch_size=8, callbacks=[early_stopping])\n",
        "model.fit(data, encoded_labels, epochs=100, batch_size=8, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8QeJyKegoAw",
        "outputId": "c9bab7e1-30b6-4b39-e052-bdadb35f3f40"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "109/109 [==============================] - 10s 50ms/step - loss: 3.8178 - accuracy: 0.1010\n",
            "Epoch 2/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 3.6142 - accuracy: 0.0999\n",
            "Epoch 3/100\n",
            "109/109 [==============================] - 6s 52ms/step - loss: 3.3534 - accuracy: 0.1056\n",
            "Epoch 4/100\n",
            "109/109 [==============================] - 5s 42ms/step - loss: 3.1931 - accuracy: 0.1515\n",
            "Epoch 5/100\n",
            "109/109 [==============================] - 5s 42ms/step - loss: 3.0256 - accuracy: 0.1894\n",
            "Epoch 6/100\n",
            "109/109 [==============================] - 6s 53ms/step - loss: 2.8838 - accuracy: 0.2158\n",
            "Epoch 7/100\n",
            "109/109 [==============================] - 5s 42ms/step - loss: 2.6642 - accuracy: 0.2939\n",
            "Epoch 8/100\n",
            "109/109 [==============================] - 6s 53ms/step - loss: 2.4876 - accuracy: 0.3364\n",
            "Epoch 9/100\n",
            "109/109 [==============================] - 5s 42ms/step - loss: 2.3372 - accuracy: 0.3594\n",
            "Epoch 10/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 2.2348 - accuracy: 0.3984\n",
            "Epoch 11/100\n",
            "109/109 [==============================] - 6s 52ms/step - loss: 2.0744 - accuracy: 0.4466\n",
            "Epoch 12/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 1.9547 - accuracy: 0.4684\n",
            "Epoch 13/100\n",
            "109/109 [==============================] - 6s 53ms/step - loss: 1.8833 - accuracy: 0.5086\n",
            "Epoch 14/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 1.7495 - accuracy: 0.5580\n",
            "Epoch 15/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 1.7284 - accuracy: 0.5614\n",
            "Epoch 16/100\n",
            "109/109 [==============================] - 6s 52ms/step - loss: 1.6372 - accuracy: 0.5832\n",
            "Epoch 17/100\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 1.5266 - accuracy: 0.6292\n",
            "Epoch 18/100\n",
            "109/109 [==============================] - 6s 53ms/step - loss: 1.4944 - accuracy: 0.6211\n",
            "Epoch 19/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 1.4584 - accuracy: 0.6383\n",
            "Epoch 20/100\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 1.3906 - accuracy: 0.6774\n",
            "Epoch 21/100\n",
            "109/109 [==============================] - 6s 53ms/step - loss: 1.3329 - accuracy: 0.6785\n",
            "Epoch 22/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 1.2410 - accuracy: 0.7072\n",
            "Epoch 23/100\n",
            "109/109 [==============================] - 6s 53ms/step - loss: 1.2278 - accuracy: 0.7313\n",
            "Epoch 24/100\n",
            "109/109 [==============================] - 5s 42ms/step - loss: 1.1732 - accuracy: 0.7417\n",
            "Epoch 25/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 1.1417 - accuracy: 0.7669\n",
            "Epoch 26/100\n",
            "109/109 [==============================] - 6s 53ms/step - loss: 1.1156 - accuracy: 0.7784\n",
            "Epoch 27/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 1.1125 - accuracy: 0.7750\n",
            "Epoch 28/100\n",
            "109/109 [==============================] - 6s 53ms/step - loss: 1.0993 - accuracy: 0.7635\n",
            "Epoch 29/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 1.0362 - accuracy: 0.7956\n",
            "Epoch 30/100\n",
            "109/109 [==============================] - 5s 42ms/step - loss: 1.0063 - accuracy: 0.8025\n",
            "Epoch 31/100\n",
            "109/109 [==============================] - 6s 54ms/step - loss: 1.0096 - accuracy: 0.8037\n",
            "Epoch 32/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 0.9541 - accuracy: 0.8197\n",
            "Epoch 33/100\n",
            "109/109 [==============================] - 6s 53ms/step - loss: 0.9339 - accuracy: 0.8370\n",
            "Epoch 34/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 1.0069 - accuracy: 0.8002\n",
            "Epoch 35/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 0.9021 - accuracy: 0.8439\n",
            "Epoch 36/100\n",
            "109/109 [==============================] - 6s 53ms/step - loss: 0.9139 - accuracy: 0.8232\n",
            "Epoch 37/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 0.8998 - accuracy: 0.8473\n",
            "Epoch 38/100\n",
            "109/109 [==============================] - 6s 53ms/step - loss: 0.8456 - accuracy: 0.8565\n",
            "Epoch 39/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 0.8256 - accuracy: 0.8553\n",
            "Epoch 40/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 0.8295 - accuracy: 0.8507\n",
            "Epoch 41/100\n",
            "109/109 [==============================] - 6s 53ms/step - loss: 0.8245 - accuracy: 0.8588\n",
            "Epoch 42/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 0.8013 - accuracy: 0.8749\n",
            "Epoch 43/100\n",
            "109/109 [==============================] - 6s 53ms/step - loss: 0.7698 - accuracy: 0.8772\n",
            "Epoch 44/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 0.7923 - accuracy: 0.8680\n",
            "Epoch 45/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 0.7922 - accuracy: 0.8680\n",
            "Epoch 46/100\n",
            "109/109 [==============================] - 6s 53ms/step - loss: 0.7682 - accuracy: 0.8840\n",
            "Epoch 47/100\n",
            "109/109 [==============================] - 5s 42ms/step - loss: 0.7468 - accuracy: 0.8806\n",
            "Epoch 48/100\n",
            "109/109 [==============================] - 6s 53ms/step - loss: 0.7553 - accuracy: 0.8898\n",
            "Epoch 49/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 0.7292 - accuracy: 0.8978\n",
            "Epoch 50/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 0.7170 - accuracy: 0.8921\n",
            "Epoch 51/100\n",
            "109/109 [==============================] - 6s 53ms/step - loss: 0.7227 - accuracy: 0.9024\n",
            "Epoch 52/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 0.7395 - accuracy: 0.8749\n",
            "Epoch 53/100\n",
            "109/109 [==============================] - 6s 51ms/step - loss: 0.7690 - accuracy: 0.8726\n",
            "Epoch 54/100\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 0.7250 - accuracy: 0.8898\n",
            "Epoch 55/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 0.6838 - accuracy: 0.8932\n",
            "Epoch 56/100\n",
            "109/109 [==============================] - 6s 53ms/step - loss: 0.7208 - accuracy: 0.8944\n",
            "Epoch 57/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 0.7681 - accuracy: 0.8863\n",
            "Epoch 58/100\n",
            "109/109 [==============================] - 5s 49ms/step - loss: 0.6844 - accuracy: 0.9036\n",
            "Epoch 59/100\n",
            "109/109 [==============================] - 5s 46ms/step - loss: 0.6549 - accuracy: 0.9116\n",
            "Epoch 60/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 0.6474 - accuracy: 0.9093\n",
            "Epoch 61/100\n",
            "109/109 [==============================] - 6s 52ms/step - loss: 0.6464 - accuracy: 0.9059\n",
            "Epoch 62/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 0.6440 - accuracy: 0.9082\n",
            "Epoch 63/100\n",
            "109/109 [==============================] - 5s 48ms/step - loss: 0.6241 - accuracy: 0.9036\n",
            "Epoch 64/100\n",
            "109/109 [==============================] - 5s 48ms/step - loss: 0.6495 - accuracy: 0.9116\n",
            "Epoch 65/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 0.6808 - accuracy: 0.9082\n",
            "Epoch 66/100\n",
            "109/109 [==============================] - 6s 52ms/step - loss: 0.6599 - accuracy: 0.9047\n",
            "Epoch 67/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 0.6281 - accuracy: 0.9059\n",
            "Epoch 68/100\n",
            "109/109 [==============================] - 5s 46ms/step - loss: 0.6135 - accuracy: 0.9082\n",
            "Epoch 69/100\n",
            "109/109 [==============================] - 5s 49ms/step - loss: 0.6434 - accuracy: 0.9173\n",
            "Epoch 70/100\n",
            "109/109 [==============================] - 5s 42ms/step - loss: 0.6220 - accuracy: 0.9185\n",
            "Epoch 71/100\n",
            "109/109 [==============================] - 6s 53ms/step - loss: 0.5894 - accuracy: 0.9185\n",
            "Epoch 72/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 0.6005 - accuracy: 0.9150\n",
            "Epoch 73/100\n",
            "109/109 [==============================] - 5s 45ms/step - loss: 0.6432 - accuracy: 0.9047\n",
            "Epoch 74/100\n",
            "109/109 [==============================] - 6s 51ms/step - loss: 0.5686 - accuracy: 0.9300\n",
            "Epoch 75/100\n",
            "109/109 [==============================] - 5s 42ms/step - loss: 0.6122 - accuracy: 0.9185\n",
            "Epoch 76/100\n",
            "109/109 [==============================] - 6s 53ms/step - loss: 0.6002 - accuracy: 0.9162\n",
            "Epoch 77/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 0.5910 - accuracy: 0.9116\n",
            "Epoch 78/100\n",
            "109/109 [==============================] - 5s 45ms/step - loss: 0.5568 - accuracy: 0.9265\n",
            "Epoch 79/100\n",
            "109/109 [==============================] - 6s 52ms/step - loss: 0.6127 - accuracy: 0.9162\n",
            "Epoch 80/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 0.6015 - accuracy: 0.9150\n",
            "Epoch 81/100\n",
            "109/109 [==============================] - 6s 54ms/step - loss: 0.5901 - accuracy: 0.9185\n",
            "Epoch 82/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 0.5596 - accuracy: 0.9403\n",
            "Epoch 83/100\n",
            "109/109 [==============================] - 5s 45ms/step - loss: 0.5725 - accuracy: 0.9196\n",
            "Epoch 84/100\n",
            "109/109 [==============================] - 6s 51ms/step - loss: 0.5586 - accuracy: 0.9219\n",
            "Epoch 85/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 0.5572 - accuracy: 0.9185\n",
            "Epoch 86/100\n",
            "109/109 [==============================] - 6s 54ms/step - loss: 0.5797 - accuracy: 0.9139\n",
            "Epoch 87/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 0.5973 - accuracy: 0.9139\n",
            "Epoch 88/100\n",
            "109/109 [==============================] - 5s 45ms/step - loss: 0.5792 - accuracy: 0.9162\n",
            "Epoch 89/100\n",
            "109/109 [==============================] - 6s 51ms/step - loss: 0.5442 - accuracy: 0.9231\n",
            "Epoch 90/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 0.5409 - accuracy: 0.9219\n",
            "Epoch 91/100\n",
            "109/109 [==============================] - 6s 54ms/step - loss: 0.5414 - accuracy: 0.9208\n",
            "Epoch 92/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 0.5391 - accuracy: 0.9277\n",
            "Epoch 93/100\n",
            "109/109 [==============================] - 5s 45ms/step - loss: 0.5443 - accuracy: 0.9265\n",
            "Epoch 94/100\n",
            "109/109 [==============================] - 6s 51ms/step - loss: 0.5434 - accuracy: 0.9277\n",
            "Epoch 95/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 0.5328 - accuracy: 0.9323\n",
            "Epoch 96/100\n",
            "109/109 [==============================] - 6s 53ms/step - loss: 0.5651 - accuracy: 0.9208\n",
            "Epoch 97/100\n",
            "109/109 [==============================] - 5s 43ms/step - loss: 0.6449 - accuracy: 0.9059\n",
            "Epoch 98/100\n",
            "109/109 [==============================] - 5s 45ms/step - loss: 0.5709 - accuracy: 0.9231\n",
            "Epoch 99/100\n",
            "109/109 [==============================] - 5s 50ms/step - loss: 0.5929 - accuracy: 0.9070\n",
            "Epoch 100/100\n",
            "109/109 [==============================] - 5s 44ms/step - loss: 0.5431 - accuracy: 0.9323\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4e43d43fa0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load testing intents\n",
        "data_file_test = open('intents_testing.json').read()\n",
        "intents_test = json.loads(data_file_test)\n",
        "\n",
        "# Extract text and labels from testing intents\n",
        "texts_test = []\n",
        "labels_test = []\n",
        "\n",
        "for intent in intents_test['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        texts_test.append(pattern)\n",
        "        labels_test.append(intent['tag'])\n",
        "\n",
        "# Tokenize and pad the testing text data\n",
        "sequences_test = tokenizer.texts_to_sequences(texts_test)\n",
        "data_test = pad_sequences(sequences_test, maxlen=max_sequence_length)\n",
        "\n",
        "# Encode testing labels\n",
        "encoded_labels_test = le.transform(labels_test)\n",
        "\n",
        "# Evaluate the model on the training set\n",
        "train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
        "print('Train accuracy:', train_acc)\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "test_loss, test_acc = model.evaluate(data_test, encoded_labels_test, verbose=0)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5au7pNBFQ0Xr",
        "outputId": "29a95cc1-826d-4123-e9aa-80c0565c8afc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy: 0.9565772414207458\n",
            "Test accuracy: 0.5921052694320679\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}