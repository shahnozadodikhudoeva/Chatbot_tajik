{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import nltk\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifLrccbkOpSt",
        "outputId": "8c8caa16-055b-452a-8bce-9863109439d1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Load the training and testing data\n",
        "with open('intents.json') as f:\n",
        "    intents = json.load(f)\n",
        "\n",
        "with open('intents_testing.json') as f:\n",
        "    testing_intents = json.load(f)\n",
        "\n",
        "# Preprocess the data\n",
        "words = []\n",
        "labels = []\n",
        "docs_x = []\n",
        "docs_y = []\n",
        "\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        # tokenize each word in the pattern\n",
        "        wrds = nltk.word_tokenize(pattern)\n",
        "        words.extend(wrds)\n",
        "        docs_x.append(wrds)\n",
        "        docs_y.append(intent['tag'])\n",
        "\n",
        "    if intent['tag'] not in labels:\n",
        "        labels.append(intent['tag'])\n",
        "\n",
        "# Convert the words and labels to unique arrays\n",
        "words = sorted(set(words))\n",
        "labels = sorted(set(labels))\n",
        "\n",
        "# Create training and testing data sets\n",
        "training = []\n",
        "output = []\n",
        "out_empty = [0] * len(labels)\n",
        "\n",
        "for i, doc in enumerate(docs_x):\n",
        "    bag = []\n",
        "\n",
        "    for word in words:\n",
        "        if word in doc:\n",
        "            bag.append(1)\n",
        "        else:\n",
        "            bag.append(0)\n",
        "\n",
        "    output_row = out_empty[:]\n",
        "    output_row[labels.index(docs_y[i])] = 1\n",
        "\n",
        "    training.append(bag)\n",
        "    output.append(output_row)\n",
        "\n",
        "training = np.array(training)\n",
        "output = np.array(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZ2duCmPvVQc",
        "outputId": "5e575679-cd85-4e47-f6d8-0961a8777399"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "109/109 [==============================] - 93s 819ms/step - loss: 3.6140 - accuracy: 0.0746\n",
            "Epoch 2/100\n",
            "109/109 [==============================] - 80s 737ms/step - loss: 3.5057 - accuracy: 0.1079\n",
            "Epoch 3/100\n",
            "109/109 [==============================] - 81s 745ms/step - loss: 3.4774 - accuracy: 0.1079\n",
            "Epoch 4/100\n",
            "109/109 [==============================] - 80s 737ms/step - loss: 3.4825 - accuracy: 0.1079\n",
            "Epoch 5/100\n",
            "109/109 [==============================] - 82s 752ms/step - loss: 3.4727 - accuracy: 0.1079\n",
            "Epoch 6/100\n",
            "109/109 [==============================] - 83s 762ms/step - loss: 3.4748 - accuracy: 0.1079\n",
            "Epoch 7/100\n",
            "109/109 [==============================] - 81s 741ms/step - loss: 3.4760 - accuracy: 0.1079\n",
            "Epoch 8/100\n",
            "109/109 [==============================] - 81s 743ms/step - loss: 3.4698 - accuracy: 0.1079\n",
            "Epoch 9/100\n",
            "109/109 [==============================] - 81s 747ms/step - loss: 3.4730 - accuracy: 0.1079\n",
            "Epoch 10/100\n",
            "109/109 [==============================] - 81s 744ms/step - loss: 3.4682 - accuracy: 0.1079\n",
            "Epoch 11/100\n",
            "109/109 [==============================] - 81s 739ms/step - loss: 3.4660 - accuracy: 0.1091\n",
            "Epoch 12/100\n",
            "109/109 [==============================] - 80s 739ms/step - loss: 3.4621 - accuracy: 0.1148\n",
            "Epoch 13/100\n",
            "109/109 [==============================] - 84s 770ms/step - loss: 3.4580 - accuracy: 0.1160\n",
            "Epoch 14/100\n",
            "109/109 [==============================] - 81s 740ms/step - loss: 3.4582 - accuracy: 0.1148\n",
            "Epoch 15/100\n",
            "109/109 [==============================] - 81s 742ms/step - loss: 3.4552 - accuracy: 0.1148\n",
            "Epoch 16/100\n",
            "109/109 [==============================] - 81s 743ms/step - loss: 3.4490 - accuracy: 0.1171\n",
            "Epoch 17/100\n",
            "109/109 [==============================] - 82s 750ms/step - loss: 3.4304 - accuracy: 0.1171\n",
            "Epoch 18/100\n",
            "109/109 [==============================] - 81s 738ms/step - loss: 3.3567 - accuracy: 0.1297\n",
            "Epoch 19/100\n",
            "109/109 [==============================] - 81s 744ms/step - loss: 3.2807 - accuracy: 0.1435\n",
            "Epoch 20/100\n",
            "109/109 [==============================] - 81s 744ms/step - loss: 3.2271 - accuracy: 0.1458\n",
            "Epoch 21/100\n",
            "109/109 [==============================] - 82s 750ms/step - loss: 3.2155 - accuracy: 0.1493\n",
            "Epoch 22/100\n",
            "109/109 [==============================] - 81s 739ms/step - loss: 3.1949 - accuracy: 0.1493\n",
            "Epoch 23/100\n",
            "109/109 [==============================] - 80s 738ms/step - loss: 3.1879 - accuracy: 0.1481\n",
            "Epoch 24/100\n",
            "109/109 [==============================] - 81s 743ms/step - loss: 3.1674 - accuracy: 0.1515\n",
            "Epoch 25/100\n",
            "109/109 [==============================] - 82s 752ms/step - loss: 3.1567 - accuracy: 0.1538\n",
            "Epoch 26/100\n",
            "109/109 [==============================] - 81s 742ms/step - loss: 3.1474 - accuracy: 0.1596\n",
            "Epoch 27/100\n",
            "109/109 [==============================] - 81s 740ms/step - loss: 3.1474 - accuracy: 0.1584\n",
            "Epoch 28/100\n",
            "109/109 [==============================] - 81s 741ms/step - loss: 3.1253 - accuracy: 0.1561\n",
            "Epoch 29/100\n",
            "109/109 [==============================] - 82s 755ms/step - loss: 3.1220 - accuracy: 0.1561\n",
            "Epoch 30/100\n",
            "109/109 [==============================] - 81s 742ms/step - loss: 3.1343 - accuracy: 0.1584\n",
            "Epoch 31/100\n",
            "109/109 [==============================] - 81s 744ms/step - loss: 3.1022 - accuracy: 0.1584\n",
            "Epoch 32/100\n",
            "109/109 [==============================] - 81s 740ms/step - loss: 3.0995 - accuracy: 0.1596\n",
            "Epoch 33/100\n",
            "109/109 [==============================] - 82s 750ms/step - loss: 3.1072 - accuracy: 0.1653\n",
            "Epoch 34/100\n",
            "109/109 [==============================] - 80s 735ms/step - loss: 3.0932 - accuracy: 0.1573\n",
            "Epoch 35/100\n",
            "109/109 [==============================] - 81s 739ms/step - loss: 3.0814 - accuracy: 0.1630\n",
            "Epoch 36/100\n",
            "109/109 [==============================] - 81s 740ms/step - loss: 3.0766 - accuracy: 0.1607\n",
            "Epoch 37/100\n",
            "109/109 [==============================] - 82s 751ms/step - loss: 3.0861 - accuracy: 0.1550\n",
            "Epoch 38/100\n",
            "109/109 [==============================] - 81s 739ms/step - loss: 3.0707 - accuracy: 0.1630\n",
            "Epoch 39/100\n",
            "109/109 [==============================] - 81s 739ms/step - loss: 3.0733 - accuracy: 0.1596\n",
            "Epoch 40/100\n",
            "109/109 [==============================] - 80s 737ms/step - loss: 3.0627 - accuracy: 0.1630\n",
            "Epoch 41/100\n",
            "109/109 [==============================] - 80s 739ms/step - loss: 3.0522 - accuracy: 0.1676\n",
            "Epoch 42/100\n",
            "109/109 [==============================] - 82s 749ms/step - loss: 3.0577 - accuracy: 0.1676\n",
            "Epoch 43/100\n",
            "109/109 [==============================] - 81s 742ms/step - loss: 3.0500 - accuracy: 0.1688\n",
            "Epoch 44/100\n",
            "109/109 [==============================] - 81s 741ms/step - loss: 3.0422 - accuracy: 0.1676\n",
            "Epoch 45/100\n",
            "109/109 [==============================] - 80s 737ms/step - loss: 3.0370 - accuracy: 0.1734\n",
            "Epoch 46/100\n",
            "109/109 [==============================] - 81s 747ms/step - loss: 3.0298 - accuracy: 0.1630\n",
            "Epoch 47/100\n",
            "109/109 [==============================] - 81s 740ms/step - loss: 3.0125 - accuracy: 0.1676\n",
            "Epoch 48/100\n",
            "109/109 [==============================] - 81s 740ms/step - loss: 3.0133 - accuracy: 0.1642\n",
            "Epoch 49/100\n",
            "109/109 [==============================] - 80s 738ms/step - loss: 3.0107 - accuracy: 0.1699\n",
            "Epoch 50/100\n",
            "109/109 [==============================] - 81s 743ms/step - loss: 2.9852 - accuracy: 0.1803\n",
            "Epoch 51/100\n",
            "109/109 [==============================] - 81s 739ms/step - loss: 2.9987 - accuracy: 0.1871\n",
            "Epoch 52/100\n",
            "109/109 [==============================] - 80s 738ms/step - loss: 2.9772 - accuracy: 0.1780\n",
            "Epoch 53/100\n",
            "109/109 [==============================] - 81s 740ms/step - loss: 2.9532 - accuracy: 0.1803\n",
            "Epoch 54/100\n",
            "109/109 [==============================] - 80s 737ms/step - loss: 2.9450 - accuracy: 0.1825\n",
            "Epoch 55/100\n",
            "109/109 [==============================] - 81s 748ms/step - loss: 2.9340 - accuracy: 0.1894\n",
            "Epoch 56/100\n",
            "109/109 [==============================] - 81s 741ms/step - loss: 2.9307 - accuracy: 0.1917\n",
            "Epoch 57/100\n",
            "109/109 [==============================] - 81s 743ms/step - loss: 2.9209 - accuracy: 0.1929\n",
            "Epoch 58/100\n",
            "109/109 [==============================] - 81s 740ms/step - loss: 2.9144 - accuracy: 0.1917\n",
            "Epoch 59/100\n",
            "109/109 [==============================] - 82s 755ms/step - loss: 2.8982 - accuracy: 0.2044\n",
            "Epoch 60/100\n",
            "109/109 [==============================] - 81s 746ms/step - loss: 2.8994 - accuracy: 0.1998\n",
            "Epoch 61/100\n",
            "109/109 [==============================] - 81s 741ms/step - loss: 2.8884 - accuracy: 0.1929\n",
            "Epoch 62/100\n",
            "109/109 [==============================] - 81s 741ms/step - loss: 2.8870 - accuracy: 0.2021\n",
            "Epoch 63/100\n",
            "109/109 [==============================] - 82s 753ms/step - loss: 2.8847 - accuracy: 0.2055\n",
            "Epoch 64/100\n",
            "109/109 [==============================] - 81s 741ms/step - loss: 2.8655 - accuracy: 0.2044\n",
            "Epoch 65/100\n",
            "109/109 [==============================] - 80s 739ms/step - loss: 2.8875 - accuracy: 0.1952\n",
            "Epoch 66/100\n",
            "109/109 [==============================] - 81s 741ms/step - loss: 2.8588 - accuracy: 0.2021\n",
            "Epoch 67/100\n",
            "109/109 [==============================] - 81s 747ms/step - loss: 2.8356 - accuracy: 0.2101\n",
            "Epoch 68/100\n",
            "109/109 [==============================] - 81s 746ms/step - loss: 2.8360 - accuracy: 0.2090\n",
            "Epoch 69/100\n",
            "109/109 [==============================] - 81s 739ms/step - loss: 2.8332 - accuracy: 0.2158\n",
            "Epoch 70/100\n",
            "109/109 [==============================] - 80s 739ms/step - loss: 2.8252 - accuracy: 0.2147\n",
            "Epoch 71/100\n",
            "109/109 [==============================] - 81s 748ms/step - loss: 2.8170 - accuracy: 0.2250\n",
            "Epoch 72/100\n",
            "109/109 [==============================] - 81s 742ms/step - loss: 2.8170 - accuracy: 0.2250\n",
            "Epoch 73/100\n",
            "109/109 [==============================] - 82s 749ms/step - loss: 2.8092 - accuracy: 0.2204\n",
            "Epoch 74/100\n",
            "109/109 [==============================] - 82s 753ms/step - loss: 2.8185 - accuracy: 0.2239\n",
            "Epoch 75/100\n",
            "109/109 [==============================] - 82s 752ms/step - loss: 2.8500 - accuracy: 0.2067\n",
            "Epoch 76/100\n",
            "109/109 [==============================] - 83s 758ms/step - loss: 2.7822 - accuracy: 0.2319\n",
            "Epoch 77/100\n",
            "109/109 [==============================] - 80s 739ms/step - loss: 2.7821 - accuracy: 0.2296\n",
            "Epoch 78/100\n",
            "109/109 [==============================] - 81s 740ms/step - loss: 2.7753 - accuracy: 0.2262\n",
            "Epoch 79/100\n",
            "109/109 [==============================] - 84s 767ms/step - loss: 2.7621 - accuracy: 0.2388\n",
            "Epoch 80/100\n",
            "109/109 [==============================] - 81s 748ms/step - loss: 2.7623 - accuracy: 0.2308\n",
            "Epoch 81/100\n",
            "109/109 [==============================] - 82s 757ms/step - loss: 2.7528 - accuracy: 0.2308\n",
            "Epoch 82/100\n",
            "109/109 [==============================] - 82s 749ms/step - loss: 2.7430 - accuracy: 0.2342\n",
            "Epoch 83/100\n",
            "109/109 [==============================] - 82s 754ms/step - loss: 2.7446 - accuracy: 0.2181\n",
            "Epoch 84/100\n",
            "109/109 [==============================] - 81s 744ms/step - loss: 2.7388 - accuracy: 0.2308\n",
            "Epoch 85/100\n",
            "109/109 [==============================] - 83s 761ms/step - loss: 2.7050 - accuracy: 0.2423\n",
            "Epoch 86/100\n",
            "109/109 [==============================] - 81s 745ms/step - loss: 2.7119 - accuracy: 0.2365\n",
            "Epoch 87/100\n",
            "109/109 [==============================] - 82s 752ms/step - loss: 2.7186 - accuracy: 0.2331\n",
            "Epoch 88/100\n",
            "109/109 [==============================] - 83s 763ms/step - loss: 2.7080 - accuracy: 0.2411\n",
            "Epoch 89/100\n",
            "109/109 [==============================] - 82s 753ms/step - loss: 2.6818 - accuracy: 0.2365\n",
            "Epoch 90/100\n",
            "109/109 [==============================] - 83s 758ms/step - loss: 2.6685 - accuracy: 0.2503\n",
            "Epoch 91/100\n",
            "109/109 [==============================] - 81s 743ms/step - loss: 2.6755 - accuracy: 0.2411\n",
            "Epoch 92/100\n",
            "109/109 [==============================] - 82s 754ms/step - loss: 2.6543 - accuracy: 0.2468\n",
            "Epoch 93/100\n",
            "109/109 [==============================] - 83s 755ms/step - loss: 2.6547 - accuracy: 0.2503\n",
            "Epoch 94/100\n",
            "109/109 [==============================] - 83s 759ms/step - loss: 2.6528 - accuracy: 0.2457\n",
            "Epoch 95/100\n",
            "109/109 [==============================] - 81s 744ms/step - loss: 2.6745 - accuracy: 0.2388\n",
            "Epoch 96/100\n",
            "109/109 [==============================] - 82s 749ms/step - loss: 2.6332 - accuracy: 0.2537\n",
            "Epoch 97/100\n",
            "109/109 [==============================] - 84s 765ms/step - loss: 2.6151 - accuracy: 0.2606\n",
            "Epoch 98/100\n",
            "109/109 [==============================] - 81s 743ms/step - loss: 2.6135 - accuracy: 0.2549\n",
            "Epoch 99/100\n",
            "109/109 [==============================] - 82s 755ms/step - loss: 2.6189 - accuracy: 0.2411\n",
            "Epoch 100/100\n",
            "109/109 [==============================] - 81s 746ms/step - loss: 2.6298 - accuracy: 0.2537\n",
            "1/1 [==============================] - 1s 907ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 177ms/step\n",
            "1/1 [==============================] - 0s 182ms/step\n",
            "1/1 [==============================] - 0s 171ms/step\n",
            "1/1 [==============================] - 0s 174ms/step\n",
            "1/1 [==============================] - 0s 188ms/step\n",
            "1/1 [==============================] - 0s 172ms/step\n",
            "1/1 [==============================] - 0s 182ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 94ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(len(words), 64),\n",
        "    tf.keras.layers.GRU(64, dropout=0.1),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(len(labels), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(training, output, epochs=100, batch_size=8)\n",
        "\n",
        "# Evaluate the model on the testing data\n",
        "for testing_intent in testing_intents['intents']:\n",
        "    for testing_pattern in testing_intent['patterns']:\n",
        "        testing_bag = []\n",
        "        testing_wrds = nltk.word_tokenize(testing_pattern)\n",
        "        for w in words:\n",
        "            testing_bag.append(1) if w in testing_wrds else testing_bag.append(0)\n",
        "\n",
        "        testing_bag = np.array(testing_bag)\n",
        "        testing_bag = testing_bag.reshape(1, -1)\n",
        "        result = model.predict(testing_bag)\n",
        "        tag = labels[np.argmax(result)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbStcNkGyZRa",
        "outputId": "f5cb0861-70b5-48d1-d809-bc582a6121a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 68ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 158ms/step\n",
            "1/1 [==============================] - 0s 163ms/step\n",
            "1/1 [==============================] - 0s 181ms/step\n",
            "1/1 [==============================] - 0s 187ms/step\n",
            "1/1 [==============================] - 0s 151ms/step\n",
            "1/1 [==============================] - 0s 173ms/step\n",
            "1/1 [==============================] - 0s 181ms/step\n",
            "1/1 [==============================] - 0s 158ms/step\n",
            "1/1 [==============================] - 0s 182ms/step\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 107ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 73ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 72ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 78ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 75ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "Accuracy: 10.53%\n"
          ]
        }
      ],
      "source": [
        "num_correct = 0\n",
        "total = 0\n",
        "\n",
        "for testing_intent in testing_intents['intents']:\n",
        "    for testing_pattern in testing_intent['patterns']:\n",
        "        testing_bag = []\n",
        "        testing_wrds = nltk.word_tokenize(testing_pattern)\n",
        "        for w in words:\n",
        "            testing_bag.append(1) if w in testing_wrds else testing_bag.append(0)\n",
        "\n",
        "        testing_bag = np.array(testing_bag)\n",
        "        testing_bag = testing_bag.reshape(1, -1)\n",
        "        result = model.predict(testing_bag)\n",
        "        tag = labels[np.argmax(result)]\n",
        "\n",
        "        if tag == testing_intent['tag']:\n",
        "            num_correct += 1\n",
        "\n",
        "        total += 1\n",
        "\n",
        "accuracy = num_correct / total\n",
        "print(f\"Accuracy: {accuracy*100:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
